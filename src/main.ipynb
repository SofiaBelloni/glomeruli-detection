{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "HJDIpn-_8HWj"
      },
      "source": [
        "# Things to install"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "WlQ4_fPh8HWm"
      },
      "source": [
        "## Colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mY1phunv8HWm"
      },
      "outputs": [],
      "source": [
        "!apt update && apt install -y openslide-tools\n",
        "!pip install openslide-python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9xk9yXI5-2_B"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "fL9vKIbZ8HWn"
      },
      "source": [
        "## Locale"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sXDluubr8HWn"
      },
      "outputs": [],
      "source": [
        "!pip install openslide-python\n",
        "!pip install opencv-python\n",
        "!pip install imgaug\n",
        "!pip install scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "e5gSArhj8HWo"
      },
      "outputs": [],
      "source": [
        "# The path can also be read from a config file, etc.\n",
        "OPENSLIDE_PATH = r'C:\\Users\\sofia\\openslide-win64-20230414\\openslide-win64-20230414\\bin'\n",
        "\n",
        "import os\n",
        "if hasattr(os, 'add_dll_directory'):\n",
        "    # Python >= 3.8 on Windows\n",
        "    with os.add_dll_directory(OPENSLIDE_PATH):\n",
        "        import openslide\n",
        "else:\n",
        "    import openslide"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "dBKMu2758HWo"
      },
      "source": [
        "# Code"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Patches extrapolation from wsi"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## - Import & constants"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import concurrent.futures\n",
        "import glob\n",
        "import numpy as np\n",
        "from thread import process_svs_file\n",
        "import threading\n",
        "import openslide\n",
        "import cv2\n",
        "from matplotlib import pyplot as plt\n",
        "import xml.etree.ElementTree as ET\n",
        "\n",
        "path_to_images = \"../slides/\"\n",
        "path_to_annotations = \"../annotations/\"\n",
        "el_width = 2000\n",
        "el_height = 2000\n",
        "output_width = 1000\n",
        "output_height = 1000"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## - Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_annotatios(file_path):\n",
        "    # Parsa il file XML delle annotazioni\n",
        "    tree = ET.parse(file_path)\n",
        "    root = tree.getroot()\n",
        "    # Ottieni tutte le annotazioni dal file XML\n",
        "    annotations = []\n",
        "    for annotation in root.iter('Annotation'):\n",
        "        name = annotation.get('Name')\n",
        "        coordinates = []\n",
        "        for coordinate in annotation.iter('Coordinate'):\n",
        "            x = float(coordinate.get('X'))\n",
        "            y = float(coordinate.get('Y'))\n",
        "            coordinates.append((x, y))\n",
        "        annotations.append({'name': name, 'coordinates': coordinates})\n",
        "    return annotations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def is_mostly_white(image, threshold_w=0.85, threshold_p=0.98):\n",
        "    # Converti l'immagine in scala di grigi\n",
        "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # Calcola la soglia per considerare i pixel bianchi\n",
        "    pixel_threshold = int(threshold_w * 255)\n",
        "\n",
        "    # Conta i pixel bianchi nell'immagine\n",
        "    white_pixels = np.sum(gray_image >= pixel_threshold)\n",
        "\n",
        "    # Calcola la percentuale di pixel bianchi rispetto alla dimensione totale dell'immagine\n",
        "    white_percentage = white_pixels / \\\n",
        "        (gray_image.shape[0] * gray_image.shape[1])\n",
        "\n",
        "    # Verifica se la percentuale di pixel bianchi supera la soglia\n",
        "    if white_percentage >= threshold_p:\n",
        "        return True, white_percentage\n",
        "    else:\n",
        "        return False, white_percentage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_labels(labels, annotations):\n",
        "    for annotation in annotations:\n",
        "        polygon = np.array([annotation['coordinates']], dtype=np.int32)\n",
        "        cv2.fillPoly(labels, polygon, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plt_image(image, labes):\n",
        "    fig, axs = plt.subplots(1, 2)\n",
        "    # Primo subplot: labels\n",
        "    axs[0].imshow(labes)\n",
        "    axs[0].axis('off')\n",
        "    # Secondo subplot: immagine\n",
        "    axs[1].imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
        "    axs[1].axis('off')\n",
        "    # Mostra i subplot affiancati\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def extrapolate_patches(wsi, annotation, el_width, el_height, output_width, output_height):\n",
        "    # Ottieni le dimensioni dell'immagine\n",
        "    w, h = wsi.dimensions\n",
        "    label_image = np.zeros((h, w), dtype=np.uint8)\n",
        "    annotations = get_annotatios(annotation)\n",
        "    get_labels(label_image, annotations)\n",
        "\n",
        "    # Calcola il numero di righe e colonne necessarie per suddividere l'immagine\n",
        "    num_rows = h // el_height\n",
        "    num_cols = w // el_width\n",
        "\n",
        "    # Crea un'immagine di output con le stesse dimensioni dell'immagine svs\n",
        "\n",
        "    dataset = []\n",
        "    labels = []\n",
        "\n",
        "    thread_name = threading.current_thread().name\n",
        "    file = open(\"../log/thread_\" + thread_name + \".txt\", \"a\")\n",
        "    file.write(\"Sto per leggere il file wsi\\n\")\n",
        "\n",
        "    wsi = np.array(wsi.read_region((0, 0), 0, (w, h)))\n",
        "\n",
        "    file.write(\"File letto\\n\")\n",
        "    for row in range(num_rows):\n",
        "        for col in range(num_cols):\n",
        "            # for row in range(3, 5):\n",
        "            #    for col in range(58, 60):\n",
        "            # Calcola le coordinate di inizio e fine per l'immagine corrente\n",
        "            x = col * el_width\n",
        "            y = row * el_height\n",
        "            x_end = x + el_width\n",
        "            y_end = y + el_height\n",
        "\n",
        "            # Estrai l'immagine corrente\n",
        "            region = wsi[y: y_end, x: x_end]\n",
        "            image = cv2.cvtColor(region, cv2.COLOR_RGBA2BGR)\n",
        "\n",
        "            is_white, p = is_mostly_white(image)\n",
        "            if not is_white:\n",
        "                r_image = cv2.resize(\n",
        "                    image, (output_width, output_height), interpolation=cv2.INTER_CUBIC)\n",
        "                r_label_image = cv2.resize(\n",
        "                    label_image[y:y_end, x: x_end], (output_width, output_height), interpolation=cv2.INTER_CUBIC)\n",
        "\n",
        "                dataset.append(r_image)\n",
        "                labels.append(r_label_image)\n",
        "                if not ((col == num_cols-1) or (row == num_rows-1)):\n",
        "                    x_h = x + el_width // 2\n",
        "                    x_v = x\n",
        "                    x_d = x + el_width // 2\n",
        "                    y_h = y\n",
        "                    y_v = y + el_height // 2\n",
        "                    y_d = y + el_height // 2\n",
        "                    region_h = wsi[y_h: y_h + el_height,\n",
        "                                   x_h: x_h + el_width]\n",
        "                    region_v = wsi[y_v: y_v + el_height,\n",
        "                                   x_v: x_v + el_width]\n",
        "                    region_d = wsi[y_d: y_d + el_height,\n",
        "                                   x_d: x_d + el_width]\n",
        "                    image_h = cv2.cvtColor(\n",
        "                        np.array(region_h), cv2.COLOR_RGBA2BGR)\n",
        "                    image_v = cv2.cvtColor(\n",
        "                        np.array(region_v), cv2.COLOR_RGBA2BGR)\n",
        "                    image_d = cv2.cvtColor(\n",
        "                        np.array(region_d), cv2.COLOR_RGBA2BGR)\n",
        "                    is_white_h, _ = is_mostly_white(image_h)\n",
        "                    is_white_v, _ = is_mostly_white(image_v)\n",
        "                    is_white_d, _ = is_mostly_white(image_d)\n",
        "                    if not is_white_h:\n",
        "                        r_image = cv2.resize(\n",
        "                            image_h, (output_width, output_height), interpolation=cv2.INTER_CUBIC)\n",
        "                        r_label_image = cv2.resize(\n",
        "                            label_image[y_h: y_h+el_height, x_h: x_h+el_width], (output_width, output_height), interpolation=cv2.INTER_CUBIC)\n",
        "\n",
        "                        dataset.append(r_image)\n",
        "                        labels.append(r_label_image)\n",
        "\n",
        "                    if not is_white_v:\n",
        "                        r_image = cv2.resize(\n",
        "                            image_v, (output_width, output_height), interpolation=cv2.INTER_CUBIC)\n",
        "                        r_label_image = cv2.resize(\n",
        "                            label_image[y_v: y_v+el_height, x_v: x_v+el_width], (output_width, output_height), interpolation=cv2.INTER_CUBIC)\n",
        "\n",
        "                        dataset.append(r_image)\n",
        "                        labels.append(r_label_image)\n",
        "\n",
        "                    if not is_white_d:\n",
        "                        r_image = cv2.resize(\n",
        "                            image_d, (output_width, output_height), interpolation=cv2.INTER_CUBIC)\n",
        "                        r_label_image = cv2.resize(\n",
        "                            label_image[y_d: y_d+el_height, x_d: x_d+el_width], (output_width, output_height), interpolation=cv2.INTER_CUBIC)\n",
        "\n",
        "                        dataset.append(r_image)\n",
        "                        labels.append(r_label_image)\n",
        "\n",
        "    file.write(\"Wsi elaborato\\nDataset di dimensione:\" +\n",
        "               str(np.array(dataset).shape) + \"\\nLabels di dimensione:\" + str(np.array(labels).shape))\n",
        "    file.close()\n",
        "    return dataset, labels\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def process_svs_file(svs_file, path_to_annotations, path_to_images, el_width, el_height, output_width, output_height):\n",
        "    thread_name = threading.current_thread().name\n",
        "    file = open(\"../log/thread_\" + thread_name + \".txt\", \"x\")\n",
        "    file.write(\"Sono il thread\" + thread_name + \"\\n\")\n",
        "    file.write(\"Sto elaborando il file \" +\n",
        "               svs_file[len(path_to_images):-4] + \"\\n\")\n",
        "    file.close()\n",
        "    # Ottieni il percorso del file .xml corrispondente\n",
        "    annotation = path_to_annotations + \\\n",
        "        svs_file[len(path_to_images):-4] + \".xml\"\n",
        "    # Carica l'immagine svs\n",
        "    wsi = openslide.OpenSlide(svs_file)\n",
        "    d, l = extrapolate_patches(\n",
        "        wsi, annotation, el_width, el_height, output_width, output_height)\n",
        "    np.save('../slides/' +\n",
        "            svs_file[len(path_to_images):-4] + '.npy', np.array(d))\n",
        "    np.save('../annotations/' +\n",
        "            svs_file[len(path_to_images):-4] + '_label.npy', np.array(l))\n",
        "    return d, l"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## - Code\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset = []\n",
        "labels = []\n",
        "\n",
        "file = open(\"../log/job.txt\", \"x\")\n",
        "file.write(\"Il task è iniziato\\n\")\n",
        "file.close()\n",
        "\n",
        "# Ottieni la lista dei file .svs nella cartella slides\n",
        "svs_files = glob.glob(path_to_images + \"*.svs\")\n",
        "\n",
        "# Creazione di un ThreadPoolExecutor con un numero di thread desiderato\n",
        "num_threads = 9  # Numero di thread da utilizzare\n",
        "executor = concurrent.futures.ThreadPoolExecutor(max_workers=num_threads)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Lista per salvare i future restituiti dalle chiamate asincrone\n",
        "futures = []\n",
        "file = open(\"../log/job.txt\", \"a\")\n",
        "file.write(\"Lancio i thread\\n\")\n",
        "\n",
        "# Esecuzione della funzione extrapolate_patches in parallelo per ogni svs_file\n",
        "for svs_file in svs_files:\n",
        "    future = executor.submit(process_svs_file, svs_file, svs_file, path_to_annotations,\n",
        "                             path_to_images, el_width, el_height, output_width, output_height)\n",
        "    futures.append(future)\n",
        "\n",
        "file.write(\"Aspetto la fine dei thread\\n\")\n",
        "# Attendere il completamento di tutte le chiamate asincrone\n",
        "concurrent.futures.wait(futures)\n",
        "\n",
        "file.write(\"I thread hanno finito, concateno i risultati\\n\")\n",
        "# Ottenere i risultati dai future\n",
        "dataset = []\n",
        "labels = []\n",
        "for future in futures:\n",
        "    d, l = future.result()\n",
        "    dataset.extend(d)\n",
        "    labels.extend(l)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset = np.array(dataset)\n",
        "labels = np.array(labels)\n",
        "\n",
        "np.save('../slides/dataset.npy', dataset)\n",
        "np.save('../annotations/labels.npy', labels)\n",
        "\n",
        "file.write(\"Risultati salvati\\n\")\n",
        "\n",
        "file.close()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Preprocessing data and data augmentation"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## - Import and constants"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import imgaug.augmenters as iaa\n",
        "import tensorflow as tf\n",
        "from imgaug.augmentables.segmaps import SegmentationMapsOnImage\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "path_to_dataset = \"../dataset/slides/dataset.npy\"\n",
        "path_to_labels = \"../dataset/annotations/labels.npy\""
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## - Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def data_augment():\n",
        "    return iaa.Sequential([\n",
        "        iaa.Dropout((0, 0.05)),  # Remove random pixel\n",
        "        iaa.Affine(rotate=(-30, 30)),  # Rotate between -30 and 30 degreed\n",
        "        iaa.Fliplr(0.5),  # Flip with 0.5 probability\n",
        "        iaa.Crop(percent=(0, 0.2), keep_size=True),  # Random crop\n",
        "        # Add -50 to 50 to the brightness-related channels of each image\n",
        "        iaa.WithBrightnessChannels(iaa.Add((-50, 50))),\n",
        "        # Change images to grayscale and overlay them with the original image by varying strengths, effectively removing 0 to 50% of the color\n",
        "        iaa.Grayscale(alpha=(0.0, 0.5)),\n",
        "        # Add random value to each pixel\n",
        "        iaa.GammaContrast((0.5, 2.0), per_channel=True),\n",
        "        # Local distortions of images by moving points around\n",
        "        iaa.PiecewiseAffine(scale=(0.01, 0.1)),\n",
        "    ], random_order=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def process_data(image, label):\n",
        "    return tf.cast(image, tf.float32)/255, tf.one_hot(label, 2, name=\"label\", axis=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def data_aug_impl(dataset, image_train, label_train):\n",
        "    da = data_augment()\n",
        "    segmented_label_train = [SegmentationMapsOnImage(\n",
        "        label, shape=dataset[1].shape) for label in label_train]\n",
        "    image_train_copy = image_train.copy()\n",
        "    for _ in range(1):\n",
        "        augmented_images, augmented_labels = da(\n",
        "            images=image_train_copy, segmentation_maps=segmented_label_train)\n",
        "        image_train = np.append(image_train, augmented_images, axis=0)\n",
        "        label_train = np.append(label_train, np.array(\n",
        "            [label.get_arr() for label in augmented_labels]), axis=0)\n",
        "\n",
        "    return image_train, label_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_train_data_tensor(image_train, label_train):\n",
        "    train_data = tf.data.Dataset.from_tensor_slices((image_train, label_train))\n",
        "    train_data = train_data.map(\n",
        "        process_data, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    train_data = train_data.cache()\n",
        "    train_data = train_data.shuffle(100)\n",
        "    train_data = train_data.batch(128)\n",
        "    train_data = train_data.prefetch(tf.data.AUTOTUNE)\n",
        "    return train_data"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## - Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Loading dataset and labels\")\n",
        "\n",
        "dataset = np.load(path_to_dataset)[0:10]\n",
        "labels = np.load(path_to_labels)[0:10]\n",
        "\n",
        "print(\n",
        "    f\"Dataset and labels loaded\\nDataset shape {dataset.shape} \\nLabels shape {labels.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "image_train, image_test, label_train, label_test = train_test_split(\n",
        "    dataset, labels, test_size=0.25, random_state=42)\n",
        "\n",
        "print(\"Dataset and labels splitted in train and test set\\n\" +\n",
        "      f\"image_train shape {image_train.shape} - label_train shape {label_train.shape}\" +\n",
        "      f\"image_test shape {image_test.shape} - image_test shape {label_test.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "image_train, label_train = data_aug_impl(dataset, image_train, label_train)\n",
        "\n",
        "print(\"Applied data agumentation to train set\\n\" +\n",
        "      f\"image_train augmented shape {image_train.shape} - label_train augmented shape {label_train.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_data = generate_train_data_tensor(image_train, label_train)\n",
        "\n",
        "print(\"Preprocessed and created tensor dataset\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# SegNet"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## - Import and constants"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import BatchNormalization, Conv2D, UpSampling2D\n",
        "\n",
        "NUM_CLASSES = 2\n",
        "INPUT_SHAPE = (400, 400, 3)\n",
        "IMAGE_SIZE = (400, 400)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## - Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SegNet(tf.keras.models.Model):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        vgg19 = tf.keras.applications.vgg19.VGG19(\n",
        "            include_top=False,   # Exclusion of the last 3 layers\n",
        "            weights='imagenet',\n",
        "            # input_tensor=None,\n",
        "            input_shape=INPUT_SHAPE,\n",
        "            pooling='max',\n",
        "            classes=NUM_CLASSES,\n",
        "            classifier_activation='relu'\n",
        "        )\n",
        "        self.encoder = tf.keras.Sequential([\n",
        "            # vgg19.get_layer('input_2'),\n",
        "            # vgg19.get_layer('input_1'),\n",
        "            BatchNormalization(),\n",
        "            vgg19.get_layer('block1_conv1'),\n",
        "            BatchNormalization(),\n",
        "            vgg19.get_layer('block1_conv2'),\n",
        "            BatchNormalization(),\n",
        "            vgg19.get_layer('block1_pool'),\n",
        "            vgg19.get_layer('block2_conv1'),\n",
        "            BatchNormalization(),\n",
        "            vgg19.get_layer('block2_conv2'),\n",
        "            BatchNormalization(),\n",
        "            vgg19.get_layer('block2_pool'),\n",
        "            vgg19.get_layer('block3_conv1'),\n",
        "            BatchNormalization(),\n",
        "            vgg19.get_layer('block3_conv2'),\n",
        "            BatchNormalization(),\n",
        "            vgg19.get_layer('block3_conv3'),\n",
        "            BatchNormalization(),\n",
        "            vgg19.get_layer('block3_conv4'),\n",
        "            BatchNormalization(),\n",
        "            vgg19.get_layer('block3_pool'),\n",
        "            vgg19.get_layer('block4_conv1'),\n",
        "            BatchNormalization(),\n",
        "            vgg19.get_layer('block4_conv2'),\n",
        "            BatchNormalization(),\n",
        "            vgg19.get_layer('block4_conv3'),\n",
        "            BatchNormalization(),\n",
        "            vgg19.get_layer('block4_conv4'),\n",
        "            BatchNormalization(),\n",
        "            vgg19.get_layer('block4_pool')\n",
        "        ])\n",
        "\n",
        "        self.decoder = tf.keras.Sequential([\n",
        "            # Block 5\n",
        "            UpSampling2D(size=(2, 2)),\n",
        "            Conv2D(512, (3, 3), activation='relu',\n",
        "                   padding='same'),  # TODO check padding\n",
        "            BatchNormalization(),\n",
        "            Conv2D(512, (3, 3), activation='relu', padding='same'),\n",
        "            BatchNormalization(),\n",
        "            Conv2D(512, (3, 3), activation='relu', padding='same'),\n",
        "            BatchNormalization(),\n",
        "            Conv2D(512, (3, 3), activation='relu', padding='same'),\n",
        "            BatchNormalization(),\n",
        "            # Block 4\n",
        "            UpSampling2D(size=(2, 2)),\n",
        "            Conv2D(512, (3, 3), activation='relu', padding='same'),\n",
        "            BatchNormalization(),\n",
        "            Conv2D(512, (3, 3), activation='relu', padding='same'),\n",
        "            BatchNormalization(),\n",
        "            Conv2D(512, (3, 3), activation='relu', padding='same'),\n",
        "            BatchNormalization(),\n",
        "            Conv2D(512, (3, 3), activation='relu', padding='same'),\n",
        "            BatchNormalization(),\n",
        "            # Block 3\n",
        "            UpSampling2D(size=(2, 2)),\n",
        "            Conv2D(256, (3, 3), activation='relu', padding='same'),\n",
        "            BatchNormalization(),\n",
        "            Conv2D(256, (3, 3), activation='relu', padding='same'),\n",
        "            BatchNormalization(),\n",
        "            Conv2D(256, (3, 3), activation='relu', padding='same'),\n",
        "            BatchNormalization(),\n",
        "            Conv2D(256, (3, 3), activation='relu', padding='same'),\n",
        "            BatchNormalization(),\n",
        "            # Block 2\n",
        "            UpSampling2D(size=(2, 2)),\n",
        "            Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
        "            BatchNormalization(),\n",
        "            Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
        "            BatchNormalization(),\n",
        "            # Block 1\n",
        "            UpSampling2D(size=(2, 2)),\n",
        "            Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
        "            BatchNormalization(),\n",
        "            Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
        "            BatchNormalization(),\n",
        "            # Softmax\n",
        "            Conv2D(NUM_CLASSES, (1, 1), activation='softmax', padding='valid'),\n",
        "        ])\n",
        "\n",
        "    def call(self, input):\n",
        "        output = self.decoder(self.encoder(input))\n",
        "        resized_output = tf.image.resize(output, IMAGE_SIZE)\n",
        "        return resized_output"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## - Code "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "segnet = SegNet()\n",
        "print(\"Created segnet model\")\n",
        "\n",
        "optimizer = tf.keras.optimizers.legacy.SGD(learning_rate=0.01)\n",
        "segnet.compile(optimizer=optimizer,\n",
        "               loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "print(\"Starting training\")\n",
        "history = segnet.fit(train_data, epochs=3)\n",
        "print(\"Training completed\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
