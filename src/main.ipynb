{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "HJDIpn-_8HWj"
      },
      "source": [
        "# Things to install\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "WlQ4_fPh8HWm"
      },
      "source": [
        "## Colab\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mY1phunv8HWm"
      },
      "outputs": [],
      "source": [
        "!apt update & & apt install - y openslide-tools\n",
        "!pip install openslide-python\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9xk9yXI5-2_B",
        "outputId": "d2491e54-0696-4e2a-dc92-a9629b968285"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "fL9vKIbZ8HWn"
      },
      "source": [
        "## Locale\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sXDluubr8HWn"
      },
      "outputs": [],
      "source": [
        "!pip install openslide-python\n",
        "!pip install opencv-python\n",
        "!pip install imgaug\n",
        "!pip install scikit-learn\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zcqvMMLzk9zN"
      },
      "outputs": [],
      "source": [
        "!pip install scikit-learn\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e5gSArhj8HWo"
      },
      "outputs": [],
      "source": [
        "# The path can also be read from a config file, etc.\n",
        "import os\n",
        "OPENSLIDE_PATH = r'C:\\Users\\sofia\\openslide-win64-20230414\\openslide-win64-20230414\\bin'\n",
        "\n",
        "if hasattr(os, 'add_dll_directory'):\n",
        "    # Python >= 3.8 on Windows\n",
        "    with os.add_dll_directory(OPENSLIDE_PATH):\n",
        "        import openslide\n",
        "else:\n",
        "    import openslide\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "dBKMu2758HWo"
      },
      "source": [
        "# Code\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "U8eRJygYk9zO"
      },
      "source": [
        "# Patches extrapolation from wsi\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "3y8mVZWCk9zO"
      },
      "source": [
        "## - Import & constants\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PJrWQaXgk9zO"
      },
      "outputs": [],
      "source": [
        "import concurrent.futures\n",
        "import glob\n",
        "import numpy as np\n",
        "from thread import process_svs_file\n",
        "import threading\n",
        "import openslide\n",
        "import cv2\n",
        "from matplotlib import pyplot as plt\n",
        "import xml.etree.ElementTree as ET\n",
        "\n",
        "path_to_images = \"../slides/\"\n",
        "path_to_annotations = \"../annotations/\"\n",
        "el_width = 2000\n",
        "el_height = 2000\n",
        "output_width = 1000\n",
        "output_height = 1000\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "RnJZqoTdk9zP"
      },
      "source": [
        "## - Function\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "71-sqETgk9zP"
      },
      "outputs": [],
      "source": [
        "def get_annotatios(file_path):\n",
        "    # Parsa il file XML delle annotazioni\n",
        "    tree = ET.parse(file_path)\n",
        "    root = tree.getroot()\n",
        "    # Ottieni tutte le annotazioni dal file XML\n",
        "    annotations = []\n",
        "    for annotation in root.iter('Annotation'):\n",
        "        name = annotation.get('Name')\n",
        "        coordinates = []\n",
        "        for coordinate in annotation.iter('Coordinate'):\n",
        "            x = float(coordinate.get('X'))\n",
        "            y = float(coordinate.get('Y'))\n",
        "            coordinates.append((x, y))\n",
        "        annotations.append({'name': name, 'coordinates': coordinates})\n",
        "    return annotations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "msc05CYVk9zQ"
      },
      "outputs": [],
      "source": [
        "def is_mostly_white(image, threshold_w=0.85, threshold_p=0.98):\n",
        "    # Converti l'immagine in scala di grigi\n",
        "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # Calcola la soglia per considerare i pixel bianchi\n",
        "    pixel_threshold = int(threshold_w * 255)\n",
        "\n",
        "    # Conta i pixel bianchi nell'immagine\n",
        "    white_pixels = np.sum(gray_image >= pixel_threshold)\n",
        "\n",
        "    # Calcola la percentuale di pixel bianchi rispetto alla dimensione totale dell'immagine\n",
        "    white_percentage = white_pixels / \\\n",
        "        (gray_image.shape[0] * gray_image.shape[1])\n",
        "\n",
        "    # Verifica se la percentuale di pixel bianchi supera la soglia\n",
        "    if white_percentage >= threshold_p:\n",
        "        return True, white_percentage\n",
        "    else:\n",
        "        return False, white_percentage\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e5qytRJUk9zQ"
      },
      "outputs": [],
      "source": [
        "def get_labels(labels, annotations):\n",
        "    for annotation in annotations:\n",
        "        polygon = np.array([annotation['coordinates']], dtype=np.int32)\n",
        "        cv2.fillPoly(labels, polygon, 1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fuczo9Gdk9zQ"
      },
      "outputs": [],
      "source": [
        "def plt_image(image, labes):\n",
        "    fig, axs = plt.subplots(1, 2)\n",
        "    # Primo subplot: labels\n",
        "    axs[0].imshow(labes)\n",
        "    axs[0].axis('off')\n",
        "    # Secondo subplot: immagine\n",
        "    axs[1].imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
        "    axs[1].axis('off')\n",
        "    # Mostra i subplot affiancati\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s_JFx6Uak9zQ"
      },
      "outputs": [],
      "source": [
        "def extrapolate_patches(wsi, annotation, el_width, el_height, output_width, output_height):\n",
        "    # Ottieni le dimensioni dell'immagine\n",
        "    w, h = wsi.dimensions\n",
        "    label_image = np.zeros((h, w), dtype=np.uint8)\n",
        "    annotations = get_annotatios(annotation)\n",
        "    get_labels(label_image, annotations)\n",
        "\n",
        "    # Calcola il numero di righe e colonne necessarie per suddividere l'immagine\n",
        "    num_rows = h // el_height\n",
        "    num_cols = w // el_width\n",
        "\n",
        "    # Crea un'immagine di output con le stesse dimensioni dell'immagine svs\n",
        "\n",
        "    dataset = []\n",
        "    labels = []\n",
        "\n",
        "    thread_name = threading.current_thread().name\n",
        "    file = open(\"../log/thread_\" + thread_name + \".txt\", \"a\")\n",
        "    file.write(\"Sto per leggere il file wsi\\n\")\n",
        "\n",
        "    wsi = np.array(wsi.read_region((0, 0), 0, (w, h)))\n",
        "\n",
        "    file.write(\"File letto\\n\")\n",
        "    for row in range(num_rows):\n",
        "        for col in range(num_cols):\n",
        "            # for row in range(3, 5):\n",
        "            #    for col in range(58, 60):\n",
        "            # Calcola le coordinate di inizio e fine per l'immagine corrente\n",
        "            x = col * el_width\n",
        "            y = row * el_height\n",
        "            x_end = x + el_width\n",
        "            y_end = y + el_height\n",
        "\n",
        "            # Estrai l'immagine corrente\n",
        "            region = wsi[y: y_end, x: x_end]\n",
        "            image = cv2.cvtColor(region, cv2.COLOR_RGBA2BGR)\n",
        "\n",
        "            is_white, p = is_mostly_white(image)\n",
        "            if not is_white:\n",
        "                r_image = cv2.resize(\n",
        "                    image, (output_width, output_height), interpolation=cv2.INTER_CUBIC)\n",
        "                r_label_image = cv2.resize(\n",
        "                    label_image[y:y_end, x: x_end], (output_width, output_height), interpolation=cv2.INTER_CUBIC)\n",
        "\n",
        "                dataset.append(r_image)\n",
        "                labels.append(r_label_image)\n",
        "                if not ((col == num_cols-1) or (row == num_rows-1)):\n",
        "                    x_h = x + el_width // 2\n",
        "                    x_v = x\n",
        "                    x_d = x + el_width // 2\n",
        "                    y_h = y\n",
        "                    y_v = y + el_height // 2\n",
        "                    y_d = y + el_height // 2\n",
        "                    region_h = wsi[y_h: y_h + el_height,\n",
        "                                   x_h: x_h + el_width]\n",
        "                    region_v = wsi[y_v: y_v + el_height,\n",
        "                                   x_v: x_v + el_width]\n",
        "                    region_d = wsi[y_d: y_d + el_height,\n",
        "                                   x_d: x_d + el_width]\n",
        "                    image_h = cv2.cvtColor(\n",
        "                        np.array(region_h), cv2.COLOR_RGBA2BGR)\n",
        "                    image_v = cv2.cvtColor(\n",
        "                        np.array(region_v), cv2.COLOR_RGBA2BGR)\n",
        "                    image_d = cv2.cvtColor(\n",
        "                        np.array(region_d), cv2.COLOR_RGBA2BGR)\n",
        "                    is_white_h, _ = is_mostly_white(image_h)\n",
        "                    is_white_v, _ = is_mostly_white(image_v)\n",
        "                    is_white_d, _ = is_mostly_white(image_d)\n",
        "                    if not is_white_h:\n",
        "                        r_image = cv2.resize(\n",
        "                            image_h, (output_width, output_height), interpolation=cv2.INTER_CUBIC)\n",
        "                        r_label_image = cv2.resize(\n",
        "                            label_image[y_h: y_h+el_height, x_h: x_h+el_width], (output_width, output_height), interpolation=cv2.INTER_CUBIC)\n",
        "\n",
        "                        dataset.append(r_image)\n",
        "                        labels.append(r_label_image)\n",
        "\n",
        "                    if not is_white_v:\n",
        "                        r_image = cv2.resize(\n",
        "                            image_v, (output_width, output_height), interpolation=cv2.INTER_CUBIC)\n",
        "                        r_label_image = cv2.resize(\n",
        "                            label_image[y_v: y_v+el_height, x_v: x_v+el_width], (output_width, output_height), interpolation=cv2.INTER_CUBIC)\n",
        "\n",
        "                        dataset.append(r_image)\n",
        "                        labels.append(r_label_image)\n",
        "\n",
        "                    if not is_white_d:\n",
        "                        r_image = cv2.resize(\n",
        "                            image_d, (output_width, output_height), interpolation=cv2.INTER_CUBIC)\n",
        "                        r_label_image = cv2.resize(\n",
        "                            label_image[y_d: y_d+el_height, x_d: x_d+el_width], (output_width, output_height), interpolation=cv2.INTER_CUBIC)\n",
        "\n",
        "                        dataset.append(r_image)\n",
        "                        labels.append(r_label_image)\n",
        "\n",
        "    file.write(\"Wsi elaborato\\nDataset di dimensione:\" +\n",
        "               str(np.array(dataset).shape) + \"\\nLabels di dimensione:\" + str(np.array(labels).shape))\n",
        "    file.close()\n",
        "    return dataset, labels\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VJ-YeUHCk9zR"
      },
      "outputs": [],
      "source": [
        "def process_svs_file(svs_file, path_to_annotations, path_to_images, el_width, el_height, output_width, output_height):\n",
        "    thread_name = threading.current_thread().name\n",
        "    file = open(\"../log/thread_\" + thread_name + \".txt\", \"x\")\n",
        "    file.write(\"Sono il thread\" + thread_name + \"\\n\")\n",
        "    file.write(\"Sto elaborando il file \" +\n",
        "               svs_file[len(path_to_images):-4] + \"\\n\")\n",
        "    file.close()\n",
        "    # Ottieni il percorso del file .xml corrispondente\n",
        "    annotation = path_to_annotations + \\\n",
        "        svs_file[len(path_to_images):-4] + \".xml\"\n",
        "    # Carica l'immagine svs\n",
        "    wsi = openslide.OpenSlide(svs_file)\n",
        "    d, l = extrapolate_patches(\n",
        "        wsi, annotation, el_width, el_height, output_width, output_height)\n",
        "    np.save('../slides/' +\n",
        "            svs_file[len(path_to_images):-4] + '.npy', np.array(d))\n",
        "    np.save('../annotations/' +\n",
        "            svs_file[len(path_to_images):-4] + '_label.npy', np.array(l))\n",
        "    return d, l\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "A7IBFpExk9zR"
      },
      "source": [
        "## - Code\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "40Lj-R_Mk9zS"
      },
      "outputs": [],
      "source": [
        "dataset = []\n",
        "labels = []\n",
        "\n",
        "file = open(\"../log/job.txt\", \"x\")\n",
        "file.write(\"Il task è iniziato\\n\")\n",
        "file.close()\n",
        "\n",
        "# Ottieni la lista dei file .svs nella cartella slides\n",
        "svs_files = glob.glob(path_to_images + \"*.svs\")\n",
        "\n",
        "# Creazione di un ThreadPoolExecutor con un numero di thread desiderato\n",
        "num_threads = 9  # Numero di thread da utilizzare\n",
        "executor = concurrent.futures.ThreadPoolExecutor(max_workers=num_threads)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hWCNJMpek9zS"
      },
      "outputs": [],
      "source": [
        "# Lista per salvare i future restituiti dalle chiamate asincrone\n",
        "futures = []\n",
        "file = open(\"../log/job.txt\", \"a\")\n",
        "file.write(\"Lancio i thread\\n\")\n",
        "\n",
        "# Esecuzione della funzione extrapolate_patches in parallelo per ogni svs_file\n",
        "for svs_file in svs_files:\n",
        "    future = executor.submit(process_svs_file, svs_file, svs_file, path_to_annotations,\n",
        "                             path_to_images, el_width, el_height, output_width, output_height)\n",
        "    futures.append(future)\n",
        "\n",
        "file.write(\"Aspetto la fine dei thread\\n\")\n",
        "# Attendere il completamento di tutte le chiamate asincrone\n",
        "concurrent.futures.wait(futures)\n",
        "\n",
        "file.write(\"I thread hanno finito, concateno i risultati\\n\")\n",
        "# Ottenere i risultati dai future\n",
        "dataset = []\n",
        "labels = []\n",
        "for future in futures:\n",
        "    d, l = future.result()\n",
        "    dataset.extend(d)\n",
        "    labels.extend(l)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "grSCpoabk9zS"
      },
      "outputs": [],
      "source": [
        "dataset = np.array(dataset)\n",
        "labels = np.array(labels)\n",
        "\n",
        "np.save('../slides/dataset.npy', dataset)\n",
        "np.save('../annotations/labels.npy', labels)\n",
        "\n",
        "file.write(\"Risultati salvati\\n\")\n",
        "\n",
        "file.close()\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "yK_eKP1Sk9zS"
      },
      "source": [
        "# Preprocessing data and data augmentation\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "4kzgKDWyk9zS"
      },
      "source": [
        "## - Import and constants\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "rHEAndCRk9zS"
      },
      "outputs": [],
      "source": [
        "import imgaug.augmenters as iaa\n",
        "import tensorflow as tf\n",
        "from imgaug.augmentables.segmaps import SegmentationMapsOnImage\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "path_to_dataset = \"/content/drive/MyDrive/Polito/MLinAP/RECHERCHE-015.npy\"\n",
        "path_to_labels = \"/content/drive/MyDrive/Polito/MLinAP/RECHERCHE-015_label.npy\"\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "iJAH0-vRk9zT"
      },
      "source": [
        "## - Function\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "5BNEJ6G4k9zT"
      },
      "outputs": [],
      "source": [
        "def data_augment():\n",
        "    return iaa.Sequential([\n",
        "        iaa.Dropout((0, 0.05)),  # Remove random pixel\n",
        "        iaa.Affine(rotate=(-30, 30)),  # Rotate between -30 and 30 degreed\n",
        "        iaa.Fliplr(0.5),  # Flip with 0.5 probability\n",
        "        iaa.Crop(percent=(0, 0.2), keep_size=True),  # Random crop\n",
        "        # Add -50 to 50 to the brightness-related channels of each image\n",
        "        iaa.WithBrightnessChannels(iaa.Add((-50, 50))),\n",
        "        # Change images to grayscale and overlay them with the original image by varying strengths, effectively removing 0 to 50% of the color\n",
        "        iaa.Grayscale(alpha=(0.0, 0.5)),\n",
        "        # Add random value to each pixel\n",
        "        iaa.GammaContrast((0.5, 2.0), per_channel=True),\n",
        "        # Local distortions of images by moving points around\n",
        "        iaa.PiecewiseAffine(scale=(0.01, 0.1)),\n",
        "    ], random_order=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "fQ6MXI87k9zT"
      },
      "outputs": [],
      "source": [
        "def process_data(image, label):\n",
        "    return tf.cast(image, tf.float32)/255, tf.one_hot(label, 2, name=\"label\", axis=-1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "dj1YZS2ok9zT"
      },
      "outputs": [],
      "source": [
        "def data_aug_impl(dataset, image_train, label_train):\n",
        "    da = data_augment()\n",
        "    segmented_label_train = [SegmentationMapsOnImage(\n",
        "        label, shape=dataset[1].shape) for label in label_train]\n",
        "    image_train_copy = image_train.copy()\n",
        "    for _ in range(1):\n",
        "        augmented_images, augmented_labels = da(\n",
        "            images=image_train_copy, segmentation_maps=segmented_label_train)\n",
        "        image_train = np.append(image_train, augmented_images, axis=0)\n",
        "        label_train = np.append(label_train, np.array(\n",
        "            [label.get_arr() for label in augmented_labels]), axis=0)\n",
        "\n",
        "    return image_train, label_train\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "WkTurhUjk9zT"
      },
      "outputs": [],
      "source": [
        "def generate_train_data_tensor(image_train, label_train):\n",
        "    train_data = tf.data.Dataset.from_tensor_slices((image_train, label_train))\n",
        "    train_data = train_data.map(\n",
        "        process_data, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    train_data = train_data.cache()\n",
        "    train_data = train_data.shuffle(100)\n",
        "    train_data = train_data.batch(128)\n",
        "    train_data = train_data.prefetch(tf.data.AUTOTUNE)\n",
        "    return train_data\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "pg-HaqAqk9zU"
      },
      "source": [
        "## - Code\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "abj_6t7Jk9zU",
        "outputId": "b8568d50-d138-4c75-9e60-4066efdd1df7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading dataset and labels\n",
            "Dataset and labels loaded\n",
            "Dataset shape (2, 512, 512, 3) \n",
            "Labels shape (2, 512, 512)\n"
          ]
        }
      ],
      "source": [
        "print(\"Loading dataset and labels\")\n",
        "\n",
        "dataset = np.load(path_to_dataset)[0:2]\n",
        "labels = np.load(path_to_labels)[0:2]\n",
        "\n",
        "print(\n",
        "    f\"Dataset and labels loaded\\nDataset shape {dataset.shape} \\nLabels shape {labels.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YKxd9A1Vk9zU",
        "outputId": "c7560efd-7a10-44ae-e6f6-3fb517e19f3d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset and labels splitted in train and test set\n",
            "image_train shape (1, 512, 512, 3) - label_train shape (1, 512, 512)image_test shape (1, 512, 512, 3) - image_test shape (1, 512, 512)\n"
          ]
        }
      ],
      "source": [
        "image_train, image_test, label_train, label_test = train_test_split(\n",
        "    dataset, labels, test_size=0.25, random_state=42)\n",
        "\n",
        "print(\"Dataset and labels splitted in train and test set\\n\" +\n",
        "      f\"image_train shape {image_train.shape} - label_train shape {label_train.shape}\" +\n",
        "      f\"image_test shape {image_test.shape} - image_test shape {label_test.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gN5pJniMk9zU",
        "outputId": "2827e027-a8e7-4ce3-97b8-38ed24afbf5f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Applied data agumentation to train set\n",
            "image_train augmented shape (2, 512, 512, 3) - label_train augmented shape (2, 512, 512)\n"
          ]
        }
      ],
      "source": [
        "image_train, label_train = data_aug_impl(dataset, image_train, label_train)\n",
        "\n",
        "print(\"Applied data agumentation to train set\\n\" +\n",
        "      f\"image_train augmented shape {image_train.shape} - label_train augmented shape {label_train.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kLFH5jL-k9zU",
        "outputId": "0c30627f-ea62-469d-81aa-171c0550ce38"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Preprocessed and created tensor dataset\n"
          ]
        }
      ],
      "source": [
        "train_data = generate_train_data_tensor(image_train, label_train)\n",
        "\n",
        "print(\"Preprocessed and created tensor dataset\")\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "UJHa5XI2k9zU"
      },
      "source": [
        "# SegNet && Unet\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "4e05j6HHk9zU"
      },
      "source": [
        "## - Import and constants\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "GoRaY33rk9zU"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from keras.models import Model\n",
        "from keras.layers import BatchNormalization\n",
        "from keras.layers.convolutional import Conv2D\n",
        "from tensorflow.keras.layers import Activation\n",
        "from keras import backend as K\n",
        "from keras.layers import Layer\n",
        "from keras.layers.convolutional import Conv2D, BatchNormalization\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "import json\n",
        "\n",
        "NUM_CLASSES = 2\n",
        "INPUT_SHAPE = (512, 512, 3)\n",
        "IMAGE_SIZE = (512, 512)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "jaeEsoKOk9zc"
      },
      "source": [
        "## - Class\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## segnet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "LPNEItIFk9zd"
      },
      "outputs": [],
      "source": [
        "from keras import backend as K\n",
        "from keras.layers import Layer\n",
        "\n",
        "\n",
        "class MaxPoolingWithArgmax2D(Layer):\n",
        "    def __init__(self, pool_size=(2, 2), strides=(2, 2), padding=\"same\", **kwargs):\n",
        "        super(MaxPoolingWithArgmax2D, self).__init__(**kwargs)\n",
        "        self.padding = padding\n",
        "        self.pool_size = pool_size\n",
        "        self.strides = strides\n",
        "\n",
        "    def call(self, inputs, **kwargs):\n",
        "        padding = self.padding\n",
        "        pool_size = self.pool_size\n",
        "        strides = self.strides\n",
        "        if K.backend() == \"tensorflow\":\n",
        "            ksize = [1, pool_size[0], pool_size[1], 1]\n",
        "            padding = padding.upper()\n",
        "            strides = [1, strides[0], strides[1], 1]\n",
        "            output, argmax = K.tf.nn.max_pool_with_argmax(\n",
        "                inputs, ksize=ksize, strides=strides, padding=padding\n",
        "            )\n",
        "        else:\n",
        "            errmsg = \"{} backend is not supported for layer {}\".format(\n",
        "                K.backend(), type(self).__name__\n",
        "            )\n",
        "            raise NotImplementedError(errmsg)\n",
        "        argmax = K.cast(argmax, K.floatx())\n",
        "        return [output, argmax]\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        ratio = (1, 2, 2, 1)\n",
        "        output_shape = [\n",
        "            dim // ratio[idx] if dim is not None else None\n",
        "            for idx, dim in enumerate(input_shape)\n",
        "        ]\n",
        "        output_shape = tuple(output_shape)\n",
        "        return [output_shape, output_shape]\n",
        "\n",
        "    def compute_mask(self, inputs, mask=None):\n",
        "        return 2 * [None]\n",
        "\n",
        "\n",
        "class MaxUnpooling2D(Layer):\n",
        "    def __init__(self, size=(2, 2), **kwargs):\n",
        "        super(MaxUnpooling2D, self).__init__(**kwargs)\n",
        "        self.size = size\n",
        "\n",
        "    def call(self, inputs, output_shape=None):\n",
        "        updates, mask = inputs[0], inputs[1]\n",
        "        mask = tf.cast(mask, \"int32\")\n",
        "        input_shape = tf.shape(updates, out_type=\"int32\")\n",
        "        if output_shape is None:\n",
        "            output_shape = (\n",
        "                input_shape[0],\n",
        "                input_shape[1] * self.size[0],\n",
        "                input_shape[2] * self.size[1],\n",
        "                input_shape[3],\n",
        "            )\n",
        "        self.output_shape1 = output_shape\n",
        "\n",
        "        one_like_mask = tf.ones_like(mask, dtype=\"int32\")\n",
        "        batch_shape = tf.concat([[input_shape[0]], [1], [1], [1]], axis=0)\n",
        "        batch_range = tf.reshape(\n",
        "            tf.range(output_shape[0], dtype=\"int32\"), shape=batch_shape\n",
        "        )\n",
        "        b = one_like_mask * batch_range\n",
        "        y = mask // (output_shape[2] * output_shape[3])\n",
        "        x = (mask // output_shape[3]) % output_shape[2]\n",
        "        feature_range = tf.range(output_shape[3], dtype=\"int32\")\n",
        "        f = one_like_mask * feature_range\n",
        "\n",
        "        updates_size = tf.size(updates)\n",
        "        indices = tf.transpose(tf.reshape(\n",
        "            tf.stack([b, y, x, f]), [4, updates_size]))\n",
        "        values = tf.reshape(updates, [updates_size])\n",
        "        ret = tf.scatter_nd(indices, values, output_shape)\n",
        "        return ret\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        mask_shape = input_shape[1]\n",
        "        return (\n",
        "            mask_shape[0],\n",
        "            mask_shape[1] * self.size[0],\n",
        "            mask_shape[2] * self.size[1],\n",
        "            mask_shape[3],\n",
        "        )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LKF1fkVfk9zd"
      },
      "outputs": [],
      "source": [
        "class SegNet(Model):\n",
        "    def __init__(self, num_classes=NUM_CLASSES, input_shape=INPUT_SHAPE):\n",
        "        super().__init__()\n",
        "        vgg19 = tf.keras.applications.vgg19.VGG19(\n",
        "            include_top=False,   # Exclusion of the last 3 layers\n",
        "            weights='imagenet',\n",
        "            # input_tensor=None,\n",
        "            input_shape=input_shape,\n",
        "            pooling='max',\n",
        "            classes=num_classes,\n",
        "            classifier_activation='relu'\n",
        "        )\n",
        "        # Encoder\n",
        "        # Block 1\n",
        "        self.b1 = tf.keras.Sequential([vgg19.get_layer('block1_conv1'),\n",
        "                                      BatchNormalization(),\n",
        "                                      Activation('relu'),\n",
        "                                      vgg19.get_layer('block1_conv2'),\n",
        "                                      BatchNormalization(),\n",
        "                                      Activation('relu')])\n",
        "        self.b1p = MaxPoolingWithArgmax2D(name=\"layerMP1\")\n",
        "        # Block 2\n",
        "        self.b2 = tf.keras.Sequential([vgg19.get_layer('block2_conv1'),\n",
        "                                      BatchNormalization(),\n",
        "                                      Activation('relu'),\n",
        "                                      vgg19.get_layer('block2_conv2'),\n",
        "                                      BatchNormalization(),\n",
        "                                      Activation('relu')])\n",
        "        self.b2p = MaxPoolingWithArgmax2D(name=\"layerMP2\")\n",
        "        # Block 3\n",
        "        self.b3 = tf.keras.Sequential([vgg19.get_layer('block3_conv1'),\n",
        "                                      BatchNormalization(),\n",
        "                                      Activation('relu'),\n",
        "                                      vgg19.get_layer('block3_conv2'),\n",
        "                                      BatchNormalization(),\n",
        "                                      Activation('relu'),\n",
        "                                      vgg19.get_layer('block3_conv3'),\n",
        "                                      BatchNormalization(),\n",
        "                                      Activation('relu'),\n",
        "                                      vgg19.get_layer('block3_conv4'),\n",
        "                                      BatchNormalization(),\n",
        "                                      Activation('relu')])\n",
        "        self.b3p = MaxPoolingWithArgmax2D(name=\"layerMP3\")\n",
        "        # Block 4\n",
        "        self.b4 = tf.keras.Sequential([vgg19.get_layer('block4_conv1'),\n",
        "                                      BatchNormalization(),\n",
        "                                      Activation('relu'),\n",
        "                                      vgg19.get_layer('block4_conv2'),\n",
        "                                      BatchNormalization(),\n",
        "                                      Activation('relu'),\n",
        "                                      vgg19.get_layer('block4_conv3'),\n",
        "                                      BatchNormalization(),\n",
        "                                      Activation('relu'),\n",
        "                                      vgg19.get_layer('block4_conv4'),\n",
        "                                      BatchNormalization(),\n",
        "                                      Activation('relu')])\n",
        "        self.b4p = MaxPoolingWithArgmax2D(name=\"layerMP4\")\n",
        "        # Block 5\n",
        "        self.b5 = tf.keras.Sequential([vgg19.get_layer('block5_conv1'),\n",
        "                                      BatchNormalization(),\n",
        "                                      Activation('relu'),\n",
        "                                      vgg19.get_layer('block5_conv2'),\n",
        "                                      BatchNormalization(),\n",
        "                                      Activation('relu'),\n",
        "                                      vgg19.get_layer('block5_conv3'),\n",
        "                                      BatchNormalization(),\n",
        "                                      Activation('relu'),\n",
        "                                      vgg19.get_layer('block5_conv4'),\n",
        "                                      BatchNormalization(),\n",
        "                                      Activation('relu')])\n",
        "        self.b5p = MaxPoolingWithArgmax2D(name=\"layerMP5\")\n",
        "\n",
        "        # Decoder\n",
        "        # Block 6\n",
        "        self.b6p = MaxUnpooling2D()\n",
        "        self.b6 = tf.keras.Sequential([Conv2D(filters=512, activation='relu', kernel_size=(3, 3), kernel_initializer='he_normal', padding='same'),\n",
        "                                       BatchNormalization(),\n",
        "                                       Activation('relu'),\n",
        "                                       Conv2D(filters=512, activation='relu', kernel_size=(\n",
        "                                           3, 3), kernel_initializer='he_normal', padding='same'),\n",
        "                                       BatchNormalization(),\n",
        "                                       Activation('relu'),\n",
        "                                       Conv2D(filters=512, activation='relu', kernel_size=(\n",
        "                                           3, 3), kernel_initializer='he_normal', padding='same'),\n",
        "                                       BatchNormalization(),\n",
        "                                       Activation('relu'),\n",
        "                                       Conv2D(filters=512, activation='relu', kernel_size=(\n",
        "                                           3, 3), kernel_initializer='he_normal', padding='same'),\n",
        "                                       BatchNormalization(),\n",
        "                                       Activation('relu')])\n",
        "        # Block 7\n",
        "        self.b7p = MaxUnpooling2D()\n",
        "        self.b7 = tf.keras.Sequential([Conv2D(filters=512, activation='relu', kernel_size=(3, 3), kernel_initializer='he_normal', padding='same'),\n",
        "                                       BatchNormalization(),\n",
        "                                       Activation('relu'),\n",
        "                                       Conv2D(filters=512, activation='relu', kernel_size=(\n",
        "                                           3, 3), kernel_initializer='he_normal', padding='same'),\n",
        "                                       BatchNormalization(),\n",
        "                                       Activation('relu'),\n",
        "                                       Conv2D(filters=512, activation='relu', kernel_size=(\n",
        "                                           3, 3), kernel_initializer='he_normal', padding='same'),\n",
        "                                       BatchNormalization(),\n",
        "                                       Activation('relu'),\n",
        "                                       Conv2D(filters=256, activation='relu', kernel_size=(\n",
        "                                           3, 3), kernel_initializer='he_normal', padding='same'),\n",
        "                                       BatchNormalization(),\n",
        "                                       Activation('relu')])\n",
        "        # Block 8\n",
        "        self.b8p = MaxUnpooling2D()\n",
        "        self.b8 = tf.keras.Sequential([Conv2D(filters=256, activation='relu', kernel_size=(3, 3), kernel_initializer='he_normal', padding='same'),\n",
        "                                       BatchNormalization(),\n",
        "                                       Activation('relu'),\n",
        "                                       Conv2D(filters=256, activation='relu', kernel_size=(\n",
        "                                           3, 3), kernel_initializer='he_normal', padding='same'),\n",
        "                                       BatchNormalization(),\n",
        "                                       Activation('relu'),\n",
        "                                       Conv2D(filters=256, activation='relu', kernel_size=(\n",
        "                                           3, 3), kernel_initializer='he_normal', padding='same'),\n",
        "                                       BatchNormalization(),\n",
        "                                       Activation('relu'),\n",
        "                                       Conv2D(filters=128, activation='relu', kernel_size=(\n",
        "                                           3, 3), kernel_initializer='he_normal', padding='same'),\n",
        "                                       BatchNormalization(),\n",
        "                                       Activation('relu')])\n",
        "        # Block 9\n",
        "        self.b9p = MaxUnpooling2D()\n",
        "        self.b9 = tf.keras.Sequential([Conv2D(filters=128, activation='relu', kernel_size=(3, 3), kernel_initializer='he_normal', padding='same'),\n",
        "                                       BatchNormalization(),\n",
        "                                       Activation('relu'),\n",
        "                                       Conv2D(filters=64, activation='relu', kernel_size=(\n",
        "                                           3, 3), kernel_initializer='he_normal', padding='same'),\n",
        "                                       BatchNormalization(),\n",
        "                                       Activation('relu')])\n",
        "        # Block 10\n",
        "        self.b10p = MaxUnpooling2D()\n",
        "        self.b10 = tf.keras.Sequential([Conv2D(filters=64, activation='relu', kernel_size=(3, 3), kernel_initializer='he_normal', padding='same'),\n",
        "                                        BatchNormalization(),\n",
        "                                        Activation('relu'),\n",
        "                                        Conv2D(filters=64, activation='relu', kernel_size=(\n",
        "                                            3, 3), kernel_initializer='he_normal', padding='same'),\n",
        "                                        BatchNormalization(),\n",
        "                                        Activation('relu')])\n",
        "        self.o = Conv2D(filters=NUM_CLASSES, kernel_size=(\n",
        "            1, 1), padding='valid', activation='softmax')\n",
        "\n",
        "    def call(self, input):\n",
        "        # Encoder\n",
        "        # Block 1\n",
        "        x = self.b1(input)\n",
        "        x, index1 = self.b1p(x)\n",
        "        # Block 2\n",
        "        x = self.b2(x)\n",
        "        x, index2 = self.b2p(x)\n",
        "        # Block 3\n",
        "        x = self.b3(x)\n",
        "        x, index3 = self.b3p(x)\n",
        "        # Block 4\n",
        "        x = self.b4(x)\n",
        "        x, index4 = self.b4p(x)\n",
        "        # Block 5\n",
        "        x = self.b5(x)\n",
        "        x, index5 = self.b5p(x)\n",
        "        # Decoder\n",
        "        # Block 6\n",
        "        x = self.b6p([x, index5])\n",
        "        x = self.b6(x)\n",
        "        # Block 7\n",
        "        x = self.b7p([x, index4])\n",
        "        x = self.b7(x)\n",
        "        # Block 8\n",
        "        x = self.b8p([x, index3])\n",
        "        x = self.b8(x)\n",
        "        # Block 9\n",
        "        x = self.b9p([x, index2])\n",
        "        x = self.b9(x)\n",
        "        # Block 10\n",
        "        x = self.b10p([x, index1])\n",
        "        x = self.b10(x)\n",
        "        o = self.o(x)\n",
        "        return o\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## unet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from keras.models import Model\n",
        "from keras.layers.convolutional import Conv2D, Conv2DTranspose\n",
        "from keras.layers.merging import concatenate\n",
        "\n",
        "\n",
        "class Unet(Model):\n",
        "\n",
        "    def __init__(self, dropout_value=0.3, num_classes=2, input_shape=(512, 512, 3)):\n",
        "        super().__init__()\n",
        "        vgg19 = tf.keras.applications.vgg19.VGG19(\n",
        "            include_top=False,   # Exclusion of the last 3 layers\n",
        "            weights='imagenet',\n",
        "            # input_tensor=None,\n",
        "            input_shape=input_shape,\n",
        "            pooling='max',\n",
        "            classes=num_classes,\n",
        "            classifier_activation='relu'\n",
        "        )\n",
        "        # for layer in vgg19.layers:\n",
        "        #  layer.trainable = False\n",
        "        # Block 1\n",
        "        self.b1c1 = vgg19.get_layer('block1_conv1')\n",
        "        self.b1c2 = vgg19.get_layer('block1_conv2')\n",
        "        # Block 2\n",
        "        self.b2p = vgg19.get_layer('block1_pool')\n",
        "        self.b2c1 = vgg19.get_layer('block2_conv1')\n",
        "        self.b2c2 = vgg19.get_layer('block2_conv2')\n",
        "        # Block 3\n",
        "        self.b3p = vgg19.get_layer('block2_pool')\n",
        "        self.b3c1 = vgg19.get_layer('block3_conv1')\n",
        "        self.b3c2 = vgg19.get_layer('block3_conv2')\n",
        "        # Block 4\n",
        "        self.b4p = vgg19.get_layer('block3_pool')\n",
        "        self.b4c1 = vgg19.get_layer('block4_conv1')\n",
        "        self.b4c2 = vgg19.get_layer('block4_conv2')\n",
        "        # Block 5\n",
        "        self.b5p = vgg19.get_layer('block4_pool')\n",
        "        self.b5c1 = vgg19.get_layer('block5_conv1')\n",
        "        self.b5c2 = vgg19.get_layer('block5_conv2')\n",
        "        # Block 6\n",
        "        # self.b6d1 = Dropout(dropout_value) #Controllando meglio su internet, sembrerebbe che il dropout non è presente nativamente su unet, ma è possibile inserirlo qualora si osservi dell'overfitting\n",
        "        self.b6p = vgg19.get_layer('block5_pool')\n",
        "        self.b6c1 = Conv2D(filters=1024, activation='relu', kernel_size=(\n",
        "            3, 3), kernel_initializer='he_normal', padding='same')\n",
        "        self.b6c2 = Conv2D(filters=1024, activation='relu', kernel_size=(\n",
        "            3, 3), kernel_initializer='he_normal', padding='same')\n",
        "        # self.b6d2 = Dropout(dropout_value)\n",
        "        self.b6u = Conv2DTranspose(\n",
        "            512, (3, 3), activation=\"relu\", strides=(2, 2), padding='same')\n",
        "        # Block 7\n",
        "        # After concatenate\n",
        "        self.b7c1 = Conv2D(filters=512, activation='relu', kernel_size=(\n",
        "            3, 3), kernel_initializer='he_normal', padding='same')\n",
        "        self.b7c2 = Conv2D(filters=512, activation='relu', kernel_size=(\n",
        "            3, 3), kernel_initializer='he_normal', padding='same')\n",
        "        self.b7u = Conv2DTranspose(\n",
        "            512, (3, 3), activation=\"relu\", strides=(2, 2), padding='same')\n",
        "        # Block 8\n",
        "        # After concatenate\n",
        "        self.b8c1 = Conv2D(filters=512, activation='relu', kernel_size=(\n",
        "            3, 3), kernel_initializer='he_normal', padding='same')\n",
        "        self.b8c2 = Conv2D(filters=512, activation='relu', kernel_size=(\n",
        "            3, 3), kernel_initializer='he_normal', padding='same')\n",
        "        self.b8u = Conv2DTranspose(\n",
        "            256, (3, 3), activation=\"relu\", strides=(2, 2), padding='same')\n",
        "        # Block 9\n",
        "        # After concatenate\n",
        "        self.b9c1 = Conv2D(filters=256, activation='relu', kernel_size=(\n",
        "            3, 3), kernel_initializer='he_normal', padding='same')\n",
        "        self.b9c2 = Conv2D(filters=256, activation='relu', kernel_size=(\n",
        "            3, 3), kernel_initializer='he_normal', padding='same')\n",
        "        self.b9u = Conv2DTranspose(\n",
        "            128, (3, 3), activation=\"relu\", strides=(2, 2), padding='same')\n",
        "        # Block 10\n",
        "        # After concatenate\n",
        "        self.b10c1 = Conv2D(filters=128, activation='relu', kernel_size=(\n",
        "            3, 3), kernel_initializer='he_normal', padding='same')\n",
        "        self.b10c2 = Conv2D(filters=128, activation='relu', kernel_size=(\n",
        "            3, 3), kernel_initializer='he_normal', padding='same')\n",
        "        self.b10u = Conv2DTranspose(\n",
        "            64, (3, 3), activation=\"relu\", strides=(2, 2), padding='same')\n",
        "        # Block 11\n",
        "        # After concatenate\n",
        "        self.b11c1 = Conv2D(filters=64, activation='relu', kernel_size=(\n",
        "            3, 3), kernel_initializer='he_normal', padding='same')\n",
        "        self.b11c2 = Conv2D(filters=64, activation='relu', kernel_size=(\n",
        "            3, 3), kernel_initializer='he_normal', padding='same')\n",
        "        self.b11c3 = Conv2D(filters=64, activation='relu', kernel_size=(\n",
        "            3, 3), kernel_initializer='he_normal', padding='same')\n",
        "        self.b11s = Conv2D(2, (1, 1), activation='softmax')\n",
        "\n",
        "    def call(self, input):\n",
        "        # Block 1\n",
        "        r1 = self.b1c1(input)\n",
        "        r1 = self.b1c2(r1)\n",
        "        # Block 2\n",
        "        r2 = self.b2p(r1)\n",
        "        r2 = self.b2c1(r2)\n",
        "        r2 = self.b2c2(r2)\n",
        "        # Block 3\n",
        "        r3 = self.b3p(r2)\n",
        "        r3 = self.b3c1(r3)\n",
        "        r3 = self.b3c2(r3)\n",
        "        # Block 4\n",
        "        r4 = self.b4p(r3)\n",
        "        r4 = self.b4c1(r4)\n",
        "        r4 = self.b4c2(r4)\n",
        "        # Block 5\n",
        "        r5 = self.b5p(r4)\n",
        "        r5 = self.b5c1(r5)\n",
        "        r5 = self.b5c2(r5)\n",
        "        # Block 6\n",
        "        # r6 = self.b6d1(r5)\n",
        "        r6 = self.b6p(r5)\n",
        "        r6 = self.b6c1(r6)\n",
        "        r6 = self.b6c2(r6)\n",
        "        # r6 = self.b6d2(r6)\n",
        "        r6 = self.b6u(r6)\n",
        "        # Block 7\n",
        "        r7 = concatenate([r6, r5])\n",
        "        r7 = self.b7c1(r7)\n",
        "        r7 = self.b7c2(r7)\n",
        "        r7 = self.b7u(r7)\n",
        "        # Block 8\n",
        "        r8 = concatenate([r7, r4])\n",
        "        r8 = self.b8c1(r8)\n",
        "        r8 = self.b8c2(r8)\n",
        "        r8 = self.b8u(r8)\n",
        "        # Block 9\n",
        "        r9 = concatenate([r8, r3])\n",
        "        r9 = self.b9c1(r9)\n",
        "        r9 = self.b9c2(r9)\n",
        "        r9 = self.b9u(r9)\n",
        "        # Block 10\n",
        "        r10 = concatenate([r9, r2])\n",
        "        r10 = self.b10c1(r10)\n",
        "        r10 = self.b10c2(r10)\n",
        "        r10 = self.b10u(r10)\n",
        "        # Block 11\n",
        "        r11 = concatenate([r10, r1])\n",
        "        r11 = self.b11c1(r11)\n",
        "        r11 = self.b11c2(r11)\n",
        "        r11 = self.b11c3(r11)\n",
        "        out = self.b11s(r11)\n",
        "        return out\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "xKVZi-hik9ze"
      },
      "source": [
        "## - Code\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_data_tensor(image_train, label_train, train=True):\n",
        "    def generator():\n",
        "        for img, lbl in zip(image_train, label_train):\n",
        "            yield img, lbl\n",
        "\n",
        "    data = tf.data.Dataset.from_generator(generator,\n",
        "                                          output_signature=(\n",
        "                                              tf.TensorSpec(\n",
        "                                                  shape=(512, 512, 3), dtype=tf.float32),\n",
        "                                              tf.TensorSpec(shape=(512, 512), dtype=tf.int32)))\n",
        "    data = data.map(\n",
        "        process_data, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    data = data.cache()\n",
        "    if train:\n",
        "        data = data.shuffle(500)\n",
        "    data = data.batch(128)\n",
        "    data = data.prefetch(tf.data.AUTOTUNE)\n",
        "    return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "path_to_dataset = \"../dataset/slides/dataset.npy\"\n",
        "path_to_labels = \"../dataset/annotations/labels.npy\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "file = open(\"../log/training.txt\", \"x\")\n",
        "file.write(\"Loading dataset and labels\")\n",
        "dataset = np.load(path_to_dataset)\n",
        "labels = np.load(path_to_labels)\n",
        "file.write(\n",
        "    f\"Dataset and labels loaded\\nDataset shape {dataset.shape} \\nLabels shape {labels.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "image_train, image_vt, label_train, label_vt = train_test_split(\n",
        "    dataset, labels, test_size=0.30, random_state=42)\n",
        "image_validation, image_test, label_validation, label_test = train_test_split(\n",
        "    image_vt, label_vt, test_size=0.33, random_state=42)\n",
        "\n",
        "\n",
        "\n",
        "file.write(\"Dataset and labels splitted in train and test set\\n\" +\n",
        "      f\"image_train shape {image_train.shape} - label_train shape {label_train.shape}\" +\n",
        "      f\"image_test shape {image_test.shape} - image_test shape {label_test.shape}\")\n",
        "\n",
        "image_train, label_train = data_aug_impl(dataset, image_train, label_train)\n",
        "\n",
        "file.write(\"Applied data agumentation to train set\\n\" +\n",
        "      f\"image_train augmented shape {image_train.shape} - label_train augmented shape {label_train.shape}\")\n",
        "\n",
        "train_data = generate_data_tensor(image_train, label_train)\n",
        "validation_data = generate_data_tensor(image_validation, label_validation)\n",
        "test_data = generate_data_tensor(image_test, image_test, train=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Definisci la funzione per addestrare e valutare un modello con un dato tasso di apprendimento\n",
        "def train_and_evaluate_model(train_data, validation_data, learning_rate):\n",
        "    # Costruisci il modello\n",
        "    model = SegNet()\n",
        "    # Compila il modello con la funzione di perdita e l'ottimizzatore appropriati\n",
        "    model.compile(loss=\"cross_entropy\", optimizer=tf.keras.optimizers.Adam(learning_rate), metrics=[\"accuracy\", \"F1Score\", \"Precision\", \"Recall\", \"FalseNegatives\",\n",
        "                                                                                               \"FalsePositives\", \"TrueNegatives\", \"TruePositives\",\n",
        "                                                                                               \"cohen_kappa\", \"MatthewsCorrelationCoefficient\"])\n",
        "    # Addestra il modello sul training set\n",
        "    history = model.fit(train_data, epochs=10)\n",
        "    # Valuta il modello sul validation set\n",
        "    _, accuracy = model.evaluate(validation_data)\n",
        "    return model, accuracy, history\n",
        "\n",
        "best_accuracy = 0.0\n",
        "best_learning_rate = None\n",
        "best_models = None\n",
        "best_history = None\n",
        "\n",
        "learning_rates = [0.001, 0.01, 0.1]\n",
        "\n",
        "# Valuta ogni tasso di apprendimento e seleziona il migliore\n",
        "for learning_rate in learning_rates:\n",
        "    model, accuracy, history = train_and_evaluate_model(learning_rate)\n",
        "    print(f\"Learning Rate: {learning_rate}, Accuracy: {accuracy}\")\n",
        "    if accuracy > best_accuracy:\n",
        "        best_accuracy = accuracy\n",
        "        best_learning_rate = learning_rate\n",
        "        best_model = model\n",
        "        best_history = history\n",
        "\n",
        "\n",
        "print(f\"Best Learning Rate: {best_learning_rate}, Best Accuracy: {best_accuracy}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
