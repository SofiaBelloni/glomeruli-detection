{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(image, label):\n",
    "    return tf.cast(image, tf.float32)/255, tf.one_hot(label, 2, name=\"label\", axis=-1)\n",
    "\n",
    "\n",
    "def generate_data_tensor(image_train, label_train, train=True):\n",
    "\n",
    "    def generator():\n",
    "        for img, lbl in zip(image_train, label_train):\n",
    "            yield img, lbl\n",
    "\n",
    "    data = tf.data.Dataset.from_generator(generator,\n",
    "                                          output_signature=(\n",
    "                                              tf.TensorSpec(\n",
    "                                                  shape=(512, 512, 3), dtype=tf.float32),\n",
    "                                              tf.TensorSpec(shape=(512, 512), dtype=tf.int32)))\n",
    "    data = data.map(\n",
    "        process_data, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    if train:\n",
    "        data = data.shuffle(500)\n",
    "    data = data.cache()\n",
    "    data = data.batch(64)\n",
    "    data = data.prefetch(tf.data.AUTOTUNE)\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_train=np.load('../slides/image_train_augmented.npy')\n",
    "label_train=np.load('../annotations/label_train_augmented.npy')\n",
    "image_validation=np.load('../slides/image_validation.npy')\n",
    "label_validation=np.load('../annotations/label_validation.npy')\n",
    "image_test=np.load('../slides/image_test.npy')\n",
    "label_test=np.load('../annotations/label_test.npy')\n",
    "\n",
    "validation_data = generate_data_tensor(image_validation, label_validation)\n",
    "test_data = generate_data_tensor(image_test, image_test, train=False)\n",
    "\n",
    "\n",
    "steps_per_epoch = math.ceil(len(image_train) / 64)\n",
    "\n",
    "\n",
    "# Definisci la funzione per addestrare e valutare un modello con un dato tasso di apprendimento\n",
    "def train_and_evaluate_segnet(image_train, label_train, validation_data, learning_rate, steps_per_epoch):\n",
    "    # Costruisci il modello\n",
    "    model = SegNet()\n",
    "    # Compila il modello con la funzione di perdita e l'ottimizzatore appropriati\n",
    "    model.compile(loss=\"categorical_crossentropy\", optimizer=tf.keras.optimizers.Adam(learning_rate), metrics=[\"accuracy\", \"Precision\", \"Recall\", \"FalseNegatives\",\n",
    "                                                                                                               \"FalsePositives\", \"TrueNegatives\", \"TruePositives\"])\n",
    "    print(\"testing a model\")\n",
    "    # Addestra il modello sul training set\n",
    "    image_train_size = len(image_train) // 20\n",
    "    label_train_size = len(label_train) // 20\n",
    "    \n",
    "    for i in range(20):\n",
    "        train_data = [];\n",
    "        if i == 19:\n",
    "            train_data=generate_data_tensor(image_train=image_train[i*image_train_size:], label_train=label_train[i*label_train_size:])\n",
    "        else:\n",
    "            train_data=generate_data_tensor(image_train=image_train[i*image_train_size: (i+1)*image_train_size], label_train=label_train[i*label_train_size:(i+1)*label_train_size])\n",
    "        \n",
    "        model.fit(train_data, epochs=50, steps_per_epoch=steps_per_epoch)\n",
    "        \n",
    "  \n",
    "    # Valuta il modello sul validation set\n",
    "    evals = model.evaluate(validation_data)\n",
    "    return model, evals[1]\n",
    "\n",
    "\n",
    "def train_and_evaluate_unet(image_train, label_train, validation_data, learning_rate, steps_per_epoch):\n",
    "    # Costruisci il modello\n",
    "    model = Unet()\n",
    "    # Compila il modello con la funzione di perdita e l'ottimizzatore appropriati\n",
    "    model.compile(loss=\"categorical_crossentropy\", optimizer=tf.keras.optimizers.Adam(learning_rate), metrics=[\"accuracy\", \"Precision\", \"Recall\", \"FalseNegatives\",\n",
    "                                                                                                               \"FalsePositives\", \"TrueNegatives\", \"TruePositives\"])\n",
    "    print(\"testing a model\")\n",
    "    # Addestra il modello sul training set\n",
    "    image_train_size = len(image_train) // 20\n",
    "    label_train_size = len(label_train) // 20\n",
    "    \n",
    "    for i in range(20):\n",
    "        train_data = [];\n",
    "        if i == 19:\n",
    "            train_data=generate_data_tensor(image_train=image_train[i*image_train_size:], label_train=label_train[i*label_train_size:])\n",
    "        else:\n",
    "            train_data=generate_data_tensor(image_train=image_train[i*image_train_size: (i+1)*image_train_size], label_train=label_train[i*label_train_size:(i+1)*label_train_size])\n",
    "        \n",
    "        model.fit(train_data, epochs=50, steps_per_epoch=steps_per_epoch)\n",
    "    # Valuta il modello sul validation set\n",
    "    evals = model.evaluate(validation_data)\n",
    "    return model, evals[1]\n",
    "\n",
    "\n",
    "best_accuracy = 0.0\n",
    "best_learning_rate = None\n",
    "best_models = None\n",
    "best_history = None\n",
    "\n",
    "learning_rates = [0.001, 0.01, 0.1]\n",
    "\n",
    "\n",
    "# Valuta ogni tasso di apprendimento e seleziona il migliore\n",
    "for learning_rate in learning_rates:\n",
    "    print(\"creating a model\")\n",
    "    model, accuracy = train_and_evaluate_segnet(\n",
    "        image_train, label_train, validation_data, learning_rate, steps_per_epoch)\n",
    "    print(f\"Learning Rate: {learning_rate}, Accuracy: {accuracy}\")\n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        best_learning_rate = learning_rate\n",
    "        best_model = model\n",
    "\n",
    "best_models.save('../saved_model/segnet')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
